{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def centroid(cluster):\n",
    "    return np.mean(cluster, axis=0)\n",
    "\n",
    "def sse(data, labels):\n",
    "    res = 0\n",
    "    N = data.shape[0]  # Numero di punti dati nel dataset\n",
    "    centroid_total = np.mean(data, axis=0)  # Calcolo del centroide totale\n",
    "    total_variance = np.sum(np.square(data - centroid_total))  # Calcolo della varianza totale dei dati\n",
    "    \n",
    "    for cluster in np.unique(labels):\n",
    "        current_cluster = data[labels == cluster]\n",
    "        cluster_centroid = centroid(current_cluster)\n",
    "        cluster_variance = np.sum(np.square(current_cluster - cluster_centroid))  # Varianza dei dati nel cluster\n",
    "        res += cluster_variance * current_cluster.shape[0]  # Somma delle distanze quadrate nel cluster\n",
    "\n",
    "    sse_normalized = res /  total_variance  # Normalizzazione combinata dell'SSE\n",
    "    return sse_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset =  fetch_openml(\"pendigits\")\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "print(X.shape)\n",
    "print(X.head)\n",
    "\n",
    "print(y.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data box plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(X)\n",
    "plt.title(\"Box plot delle features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check outliers\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).sum())\n",
    "print(X.shape)\n",
    "\n",
    "#outliers removal\n",
    "Y = y[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "X = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "#data box plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(X)\n",
    "plt.title(\"Box plot delle features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()\n",
    "print(((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).sum())\n",
    "print(X.shape)\n",
    "\n",
    "#plot data distribution after the removal of outliers\n",
    "x_array = np.array(X)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = x_array[:, 0], y = x_array[:, 1],hue = Y)\n",
    "plt.xlabel('input1')\n",
    "plt.ylabel('input2')\n",
    "plt.title('Distribuzione dati originali')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data trasformazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_scaled[:, 0], y = X_scaled[:, 1],hue = Y)\n",
    "plt.xlabel('input1')\n",
    "plt.ylabel('input2')\n",
    "plt.title('distribuzione dati standardizzati')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(X_normalized)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_normalized[:, 0], y = X_normalized[:, 1],hue = Y)\n",
    "plt.xlabel('input1')\n",
    "plt.ylabel('inptu2')\n",
    "plt.title('distribuzione dati normalizzati')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset to be used for dimensionality reduction:\n",
    "#   X -> original dataset \n",
    "#   X_scaled -> standardized Dataset \n",
    "#   X_normalized -> normalized dataset\n",
    "X_or = X\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the number of principal components using the elbow rule\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "plt.figure(figsize=(20, 10))\n",
    "#plt.plot(pca.explained_variance_ratio_, marker='o',)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', )\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(0, 16, 1))\n",
    "plt.legend(['cumulative explained variance'])\n",
    "\n",
    "\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.title(\"Elbow method per PCA\")\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of 2D data of the first 2 principal components\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_pca[:, 0], y = X_pca[:, 1],hue = Y)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the number of principal components using the elbow rule\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, Y)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(np.cumsum(lda.explained_variance_ratio_), marker='o')\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(0, 16, 1))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.title(\"Elbow method per LDA\")\n",
    "plt.show()\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "X_lda = lda.fit_transform(X, Y)\n",
    "print(X_lda.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot of the data in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_lda[:, 0], y = X_lda[:, 1],hue = Y)\n",
    "plt.xlabel('LDA 1')\n",
    "plt.ylabel('LDA 2')\n",
    "plt.title('LDA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "print(X_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the data in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_tsne[:, 0], y = X_tsne[:, 1],hue = Y)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset da usare per il clustering:\n",
    "#   X -> original Dataset \n",
    "#   X_pca -> Dataset after PCA\n",
    "#   X_tsne -> Dataset after t-SNE\n",
    "#   X_lda -> Dataset after LDA\n",
    "X_test = X_tsne\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_tsne_or = tsne.fit_transform(X_or)\n",
    "X_test_or = X_tsne_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(X_test)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "#plot of the data in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test[:, 0], y = X_test[:, 1],hue = labels, palette='pastel')\n",
    "#centroids plot\n",
    "for i in range(10):\n",
    "    cen = (centroid(X_test[labels == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-means')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \", sse(X_test, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans original datas\n",
    "kmeans_or = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans_or.fit(X_test_or)\n",
    "labels_or = kmeans_or.labels_\n",
    "\n",
    "#plot of the origianl datas in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test_or[:, 0], y = X_test_or[:, 1],hue = labels_or, palette='pastel')\n",
    "#centroids plot\n",
    "for i in range(10):\n",
    "    cen = (centroid(X_test_or[labels_or == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-means dati originali')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE dati originali: \", sse(X_test_or, labels_or))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "hierarchical_clustering = AgglomerativeClustering(n_clusters=10, metric='euclidean', linkage='ward')\n",
    "hierarchical_clustering.fit(X_test)\n",
    "\n",
    "labels = hierarchical_clustering.labels_\n",
    "linked = linkage(X_test, 'ward')\n",
    "\n",
    "#plot of the data in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test[:, 0], y = X_test[:, 1],hue = labels, palette='pastel')\n",
    "#centroids plot\n",
    "for i in range(10):\n",
    "    cen = (centroid(X_test[labels == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Hierarchical Agglomerative Clustering')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \" + str(sse(X_test, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hac origianl datas\n",
    "hierarchical_clustering_or = AgglomerativeClustering(n_clusters=11, metric='euclidean', linkage='ward')\n",
    "hierarchical_clustering_or.fit(X_test_or)\n",
    "\n",
    "labels_or = hierarchical_clustering_or.labels_\n",
    "linked_or = linkage(X_test_or, 'ward')\n",
    "\n",
    "#plot dei dati originali in 2D\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test_or[:, 0], y = X_test_or[:, 1],hue = labels_or, palette='pastel')\n",
    "#centroids plot\n",
    "for i in range(10):\n",
    "    cen = (centroid(X_test_or[labels_or == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Hierarchical Agglomerative Clustering con dati originali')\n",
    "plt.show()\n",
    "\n",
    "#dendogram origianl data\n",
    "'''plt.figure(figsize=(20, 10))\n",
    "dendrogram(linked_or, orientation='top', distance_sort='descending', show_leaf_counts=False)\n",
    "plt.title('Dendrogram for Hierarchical Clustering con dati originali')\n",
    "plt.show()'''\n",
    "\n",
    "print(\"SSE dati originali: \" + str(sse(X_test_or, labels_or)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find optimal number of clusters\n",
    "sse_hca = []\n",
    "\n",
    "for N in range(2,16):\n",
    "    hierarchical_clustering = AgglomerativeClustering(n_clusters=N, metric='euclidean', linkage='ward')\n",
    "    hierarchical_clustering.fit(X_test)\n",
    "\n",
    "    labels = hierarchical_clustering.labels_\n",
    "    linked = linkage(X_test, 'ward')\n",
    "    sse_hca.append(sse(X_test,labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sse_hca, marker='o')\n",
    "plt.title(\"SSE of HAC clustering\")\n",
    "plt.xlabel(\"number of clusters\")\n",
    "plt.ylabel(\"SSE value\")\n",
    "plt.xticks(np.arange(0, 16, 1))\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN with default parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X_test)\n",
    "\n",
    "labels = dbscan.labels_\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "p = sns.scatterplot(x = X_test[:, 0], y = X_test[:, 1],hue = labels, palette='tab10')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('DBSCAN')\n",
    "p.get_legend().remove()\n",
    "for i in range(len(np.unique(labels))):\n",
    "    cen = (centroid(X_test[labels == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \" + str(sse(X_test, labels)))\n",
    "print(\"Numero di cluster: \" + str(len(np.unique(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbscan with origianl datas\n",
    "dbscan_or = DBSCAN()\n",
    "dbscan_or.fit(X_test_or)\n",
    "\n",
    "labels_or = dbscan_or.labels_\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "p = sns.scatterplot(x = X_test_or[:, 0], y = X_test_or[:, 1],hue = labels_or, palette='tab10')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('DBSCAN dati originali')\n",
    "p.get_legend().remove()\n",
    "for i in range(len(np.unique(labels_or))):\n",
    "    cen = (centroid(X_test_or[labels_or == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE dati originali: \" + str(sse(X_test_or, labels_or)))\n",
    "print(\"Numero di cluster dati originali: \" + str(len(np.unique(labels_or))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN with personalized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=8.2, min_samples=33, metric='euclidean')\n",
    "dbscan.fit(X_test)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test[:, 0], y = X_test[:, 1],hue = labels, palette='pastel')\n",
    "for i in range(len(np.unique(labels))):\n",
    "    cen = (centroid(X_test[labels == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('DBSCAN')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \" + str(sse(X_test, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=9.5, min_samples=33, metric='euclidean')\n",
    "dbscan.fit(X_test)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test[:, 0], y = X_test[:, 1],hue = labels, palette='pastel')\n",
    "for i in range(len(np.unique(labels))):\n",
    "    cen = (centroid(X_test[labels == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('DBSCAN')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \" + str(sse(X_test, labels)))\n",
    "print(\"Numero di cluster: \" + str(len(np.unique(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbscan with origianl datas\n",
    "dbscan_or = DBSCAN(eps=8.2, min_samples=33, metric='euclidean')\n",
    "dbscan_or.fit(X_test_or)\n",
    "labels_or = dbscan_or.labels_\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x = X_test_or[:, 0], y = X_test_or[:, 1],hue = labels_or, palette='pastel')\n",
    "for i in range(len(np.unique(labels_or))):\n",
    "    cen = (centroid(X_test_or[labels_or == i]))\n",
    "    plt.text(cen[0], cen[1],\"X\", fontsize=20, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('DBSCAN dati originali')\n",
    "plt.show()\n",
    "\n",
    "print(\"SSE: \" + str(sse(X_test_or, labels_or)))\n",
    "print(\"Numero di cluster: \" + str(len(np.unique(labels_or))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
